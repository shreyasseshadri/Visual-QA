{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_model(with tensorflow)","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"z7ZFQ0hJ_RFY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"3f9407fa-9b58-40a8-9f6c-f630231b3e9e","executionInfo":{"status":"ok","timestamp":1543914965443,"user_tz":-330,"elapsed":1904,"user":{"displayName":"shubham agrawal","photoUrl":"https://lh5.googleusercontent.com/-NEB60Q038rU/AAAAAAAAAAI/AAAAAAAAK_c/DS2vp9Q2qHM/s64/photo.jpg","userId":"06863136219065353465"}}},"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n","\n","#here, we loaded the mnist dataset as a one_hot encoding (only one element in array 1, others 0)\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Extracting /tmp/data/train-images-idx3-ubyte.gz\n","Extracting /tmp/data/train-labels-idx1-ubyte.gz\n","Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n","Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"}]},{"metadata":{"id":"SJ4iE__WFdRq","colab_type":"code","colab":{}},"cell_type":"code","source":["n_classes = 10        #10 classes, one for each digit\n","batch_size = 128      #size of betch that will be given at a time to our model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hqU7dk68FhcS","colab_type":"code","colab":{}},"cell_type":"code","source":["x = tf.placeholder('float', [None, 784])    #input 28x28 image converted to an array of 784 pixels(numbers)\n","y = tf.placeholder('float')                 #output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FvfqzycFFpZ6","colab_type":"code","colab":{}},"cell_type":"code","source":["def conv2d(x, W):                                                               #2 dimentional convolution, W==weights\n","    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n","\n","def maxpool2d(x):                                                               #for spatial invariance, we do pooling, here we're doing max-pooling\n","    #                        size of window         movement of window\n","    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"we6PBF-WHalo","colab_type":"code","colab":{}},"cell_type":"code","source":["def convolutional_neural_network(x):\n","    weights = {'W_conv1':tf.Variable(tf.random_normal([5,5,1,32])),             #1st hidden layer, 5x5 convulation, 1 input, 32 output/features.\n","               'W_conv2':tf.Variable(tf.random_normal([5,5,32,64])),            #initially we are making all values random\n","               'W_fc':tf.Variable(tf.random_normal([7*7*64,1024])),             #fully connected layer on compressed image (7*7)\n","               'out':tf.Variable(tf.random_normal([1024, n_classes]))}          \n","\n","    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),                    #biases are useful when all weights are zero.\n","               'b_conv2':tf.Variable(tf.random_normal([64])),                   \n","               'b_fc':tf.Variable(tf.random_normal([1024])),\n","               'out':tf.Variable(tf.random_normal([n_classes]))}\n","\n","    x = tf.reshape(x, shape=[-1, 28, 28, 1])                                    #reshaping an array of pixels to a flat image\n","\n","    conv1 = tf.nn.relu(conv2d(x, weights['W_conv1']) + biases['b_conv1'])       #rectified linear function for first conv layer\n","    conv1 = maxpool2d(conv1)\n","    \n","    conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['b_conv2'])   \n","    conv2 = maxpool2d(conv2)\n","\n","    fc = tf.reshape(conv2,[-1, 7*7*64])\n","    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n","    fc = tf.nn.dropout(fc, keep_rate)\n","\n","    output = tf.matmul(fc, weights['out'])+biases['out']\n","\n","    return output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"85NTW63Oel0F","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def train_neural_network(x):\n","    prediction = convolutional_neural_network(x)\n","    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n","    \n","    optimizer = tf.train.AdamOptimizer().minimize(cost)                         #here we are using Adam optimizer to reduce the cost\n","    \n","    hm_epochs = 10                                                              #epoch represents the no. of complete cycles...feed forward + backprop\n","    with tf.Session() as sess:                                                  #this is where our model actually starts\n","        sess.run(tf.global_variables_initializer())\n","        \n","        for epoch in range(hm_epochs):\n","            epoch_loss = 0\n","            for _ in range(int(mnist.train.num_examples/batch_size)):\n","                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n","                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n","                epoch_loss += c\n","\n","            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n","        \n","        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n","        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))                     #calculates accuracy by finding the mean over correct values\n","        print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SlSe9KxIfZw8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":208},"outputId":"8c42002d-c5c3-4a0a-c77b-e01621ef35ab","executionInfo":{"status":"ok","timestamp":1543915266200,"user_tz":-330,"elapsed":56894,"user":{"displayName":"shubham agrawal","photoUrl":"https://lh5.googleusercontent.com/-NEB60Q038rU/AAAAAAAAAAI/AAAAAAAAK_c/DS2vp9Q2qHM/s64/photo.jpg","userId":"06863136219065353465"}}},"cell_type":"code","source":["train_neural_network(x)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Epoch 0 completed out of 10 loss: 2183127.00579834\n","Epoch 1 completed out of 10 loss: 336011.6135864258\n","Epoch 2 completed out of 10 loss: 183348.78244781494\n","Epoch 3 completed out of 10 loss: 121244.92943763733\n","Epoch 4 completed out of 10 loss: 81452.38103103638\n","Epoch 5 completed out of 10 loss: 59420.420053482056\n","Epoch 6 completed out of 10 loss: 42443.262062072754\n","Epoch 7 completed out of 10 loss: 33823.78460842371\n","Epoch 8 completed out of 10 loss: 26247.725021582097\n","Epoch 9 completed out of 10 loss: 23826.8229967961\n","Accuracy: 0.9734\n"],"name":"stdout"}]}]}